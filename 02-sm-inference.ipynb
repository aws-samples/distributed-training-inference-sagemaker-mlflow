{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf8933f-9432-4fa2-9e79-c56e90116e6f",
   "metadata": {},
   "source": [
    "# Deploy a Distributed Pytorch Recommendation model in Amazon SageMaker\n",
    "Previously, we trained a two tower model that produced a user and a movie embedding table. These embedding tables allows a recommendation engine to find similar movies to the user features in the embedding space. Performing similarity search across embedding space is efficient using Cosine similarity, or Dot product or using ANN. Apart from the efficiency gain from retrieving relevant movies using an optimized similarity search algorithm, this retrieval process can be used to narrow the number of potential movies from million of titles to tens or hundreds. \n",
    "\n",
    "The retrieval process is depicted in the following diagram:\n",
    "\n",
    "<img src=\"img/two-tower-retrieval.png\" width=\"800\">\n",
    "\n",
    "The two tower system is sometimes called a 2 stage recommendation system. \n",
    "A two-stage recommendation system consists of the following component:\n",
    "\n",
    "* Candidate Generation (First Stage): Quickly retrieves thousands of relevant items from a massive catalog of millions or billions of items\n",
    "* Ranking (Second Stage): A more powerful model that precisely ranks the retrieved candidates\n",
    "\n",
    "The movie retrieval engine described above is addressed in the first stage: candidate generation. The second stage involves a ranking model to provides ranking of the candidate movies based on the relevance. Putting everything together, here's a complete two tower recommendation system in a pipeline:\n",
    "\n",
    "<img src=\"img/two-stage-retrieval-recsys.png\" width=\"800\">\n",
    "\n",
    "In this lab, we are going to focus on deploying a candidate retrieval model developed using [TorchRec](https://pytorch.org/torchrec/) framework. Here's an updated diagram with dotted lines that highlights the model we are going to be deploying in this lab.\n",
    "\n",
    "<img src=\"img/two-stage-retrieval-recsys-retrieval-only.png\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01bb865-a7b1-444e-8aea-9a93da092ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfbc10-f6f8-4b0d-971a-c8c9180880e5",
   "metadata": {},
   "source": [
    "Install sagemaker dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115a4fd-cee3-4772-a851-5776f007c9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker -q -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbf9d5-8f00-4d49-bae2-a10435d020eb",
   "metadata": {},
   "source": [
    "Import required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35982013-711f-41ab-bd03-d5012f9a312a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703c66b-c3ea-4f19-a7af-2f731cb3f643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# session = LocalSession()\n",
    "# session.config = {'local': {'local_code': True } }\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "model_data_url = sm_model_s3_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bca5d-59f3-42aa-a238-d204c1bd8c18",
   "metadata": {},
   "source": [
    "# Deploy Pytorch Model using TorchServe in SageMaker \n",
    "We've now ready to deploy the model for serving recommendations. SageMaker supports the following ways to deploy a model, depending on your use case:\n",
    "\n",
    "* For persistent, real-time endpoints that make one prediction at a time, use SageMaker AI real-time hosting services. For more information, see [Real-time inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html).\n",
    "\n",
    "* Workloads that have idle periods between traffic spikes and can tolerate cold starts, use Serverless Inference. For more informatoin, see [Deploy models with Amazon SageMaker Serverless Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html).\n",
    "\n",
    "* Requests with large payload sizes up to 1GB, long processing times, and near real-time latency requirements, use Amazon SageMaker Asynchronous Inference. For more information, see [Asynchronous inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html).\n",
    "\n",
    "* To get predictions for an entire dataset, use SageMaker AI batch transform. For more information, see [Batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) for inference with Amazon SageMaker AI.\n",
    "\n",
    "Here's a diagram that summarizes the different deployment modes described above:\n",
    "\n",
    "<img src=\"img/sagemaker-deployment-modes.png\" width=\"1000\">\n",
    "\n",
    "\n",
    "For our lab, we'll deploy a [Real-time inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html) for serving inferences using TorchServe with SageMaker SDK. \n",
    "\n",
    "**Note:** The example shown below deploys a pytorch model to an endpoint behind a single GPU instance. For advanced use cases which involves multiple GPUs or GPU instances with torchserve, please refer to [this](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-models-frameworks-torchserve.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a260b2-eef7-42c3-9742-51f032c2e69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create PyTorch model\n",
    "pytorch_model = PyTorchModel(\n",
    "    source_dir=\"src\",\n",
    "    model_data=model_data_url,\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"2.5.1\",\n",
    "    py_version=\"py311\",\n",
    "    sagemaker_session=session,\n",
    "    env={\n",
    "        \"MASTER_ADDR\" : \"localhost\", \n",
    "        \"MASTER_PORT\" : \"12356\", \n",
    "        \"CUDA_VISIBLE_DEVICES\" : \"0\",\n",
    "        \"LOCAL_RANK\" : \"0\",\n",
    "        \"WORLD_SIZE\" : \"1\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57d813-be52-474b-9e00-cfc03014b549",
   "metadata": {},
   "source": [
    "Deploy the torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb6795-ed3c-4963-bb53-4ba2f9969557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    # instance_type='local_gpu', # uncomment if running in a local mode.\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d251dc4-5fb8-4c9d-9d1a-a9dcb120e5ca",
   "metadata": {},
   "source": [
    "## SageMaker Inference Toolkit for Model Lifecycle Handling\n",
    "The [inference.py](src/inference.py) script uses SageMaker Inference toolkit standard structure for the lifecycle of the model. The script implements the following key functions that the model server used to orchestrate incoming inference requests and loading/unloading of the model:\n",
    "\n",
    "* def model_fn(model_dir) - Function that handles model loading steps.\n",
    "\n",
    "* def input_fn(input_data, content_type) - Function that transforms the input data.\n",
    "\n",
    "* def predict_fn(self, data, model) - Function that perform inference on the given data.\n",
    "\n",
    "* def output_fn(prediction, accept) - Function that transforms the response data before returning the payload to the caller.\n",
    "\n",
    "\n",
    "For more information about SageMaker Inference Toolkit, please refer to this [link](https://github.com/aws/sagemaker-inference-toolkit).\n",
    "\n",
    "Following is the code for [inference.py](src/inference.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3fdd0e-78af-4461-9b0b-08a8f85f396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load src/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4bc91-a995-4bfd-849b-599d0fafdab3",
   "metadata": {},
   "source": [
    "Wait for the endpoint to be ready to serve requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d6fe3-f445-45d3-92ad-8d36f601af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "def wait_for_endpoint(endpoint_name, timeout_minutes=30):\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    max_attempts = timeout_minutes * 2  # Check every 30 seconds\n",
    "    attempts = 0\n",
    "    \n",
    "    while attempts < max_attempts:\n",
    "        response = sagemaker_client.describe_endpoint(\n",
    "            EndpointName=endpoint_name\n",
    "        )\n",
    "        status = response['EndpointStatus']\n",
    "        \n",
    "        if status == 'InService':\n",
    "            return True\n",
    "        \n",
    "        if status in ['Failed', 'OutOfService', 'Deleting']:\n",
    "            raise Exception(f\"Endpoint failed with status: {status}\")\n",
    "            \n",
    "        time.sleep(30)\n",
    "        attempts += 1\n",
    "    \n",
    "    raise TimeoutError(f\"Endpoint did not become ready within {timeout_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a383c9f-32fa-477d-93f8-7c180d98f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_endpoint(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e642357-f1b5-4fb0-9761-fd009b92b4ac",
   "metadata": {},
   "source": [
    "Test the endpoint with random userIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f74587-4210-40b3-b494-e676f2b37a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = {\"inputs\": [1234, 534, 30]} #userIds\n",
    "response = predictor.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29746949-25a3-42fd-8f81-2d26bcc26bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229f0e69-4c96-4f71-8067-ce0d946027c2",
   "metadata": {},
   "source": [
    "Saving model information for next lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107bcdf-7d15-45aa-a135-381780af066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = predictor._get_model_names()[0]\n",
    "model_serving_data_s3_uri = predictor.sagemaker_session.sagemaker_client.describe_model(\n",
    "    ModelName=model_name\n",
    ")['PrimaryContainer']['ModelDataUrl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34673eb6-401b-4004-9c64-28febbb19369",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store model_serving_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68583391-793e-4d17-aee5-34980d9972c9",
   "metadata": {},
   "source": [
    "### Delete Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d57c1-bfaf-4807-8df7-cacfa8fa71a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab1d54-a5e1-4d11-b903-00c9ea58b436",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "Congratulations! You've completed the end to end process of deploying a Pytorch model into Torchserve using SageMaker. \n",
    "In the next lab, we'll explore [Shadow testing](https://docs.aws.amazon.com/sagemaker/latest/dg/shadow-tests.html), a unique feature i SageMaker that allows you to evaluate new model variants alongside your existing production model without impacting live traffic or end users. Go ahead and open [03-sm-inference-shadow-test.ipynb](03-sm-inference-shadow-test.ipynb) and follow the instructions in the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
