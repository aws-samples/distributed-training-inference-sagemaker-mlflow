{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf8933f-9432-4fa2-9e79-c56e90116e6f",
   "metadata": {},
   "source": [
    "# Deploy a Distributed Pytorch Recommendation model in Amazon SageMaker\n",
    "Previously, we trained a two tower model that produced a user and a movie embedding table. These embedding tables allows a recommendation engine to find similar movies to the user features in the embedding space. Performing similarity search across embedding space is efficient using Cosine similarity, or Dot product or using ANN. Apart from the efficiency gain from retrieving relevant movies using an optimized similarity search algorithm, this retrieval process can be used to narrow the number of potential movies from million of titles to tens or hundreds. \n",
    "\n",
    "The retrieval process is depicted in the following diagram:\n",
    "\n",
    "<img src=\"img/two-tower-retrieval.png\" width=\"800\">\n",
    "\n",
    "The two tower system is sometimes called a 2 stage recommendation system. \n",
    "A two-stage recommendation system consists of the following component:\n",
    "\n",
    "* Candidate Generation (First Stage): Quickly retrieves thousands of relevant items from a massive catalog of millions or billions of items\n",
    "* Ranking (Second Stage): A more powerful model that precisely ranks the retrieved candidates\n",
    "\n",
    "The movie retrieval engine described above is addressed in the first stage: candidate generation. The second stage involves a ranking model to provides ranking of the candidate movies based on the relevance. Putting everything together, here's a complete two tower recommendation system in a pipeline:\n",
    "\n",
    "<img src=\"img/two-stage-retrieval-recsys.png\" width=\"800\">\n",
    "\n",
    "In this lab, we are going to focus on deploying a candidate retrieval model developed using [TorchRec](https://pytorch.org/torchrec/) framework. Here's an updated diagram with dotted lines that highlights the model we are going to be deploying in this lab.\n",
    "\n",
    "<img src=\"img/two-stage-retrieval-recsys-retrieval-only.png\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01bb865-a7b1-444e-8aea-9a93da092ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfbc10-f6f8-4b0d-971a-c8c9180880e5",
   "metadata": {},
   "source": [
    "Install sagemaker dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115a4fd-cee3-4772-a851-5776f007c9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker -q -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbf9d5-8f00-4d49-bae2-a10435d020eb",
   "metadata": {},
   "source": [
    "Import required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35982013-711f-41ab-bd03-d5012f9a312a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703c66b-c3ea-4f19-a7af-2f731cb3f643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# session = LocalSession()\n",
    "# session.config = {'local': {'local_code': True } }\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "role = get_execution_role()\n",
    "model_data_url = sm_model_s3_url\n",
    "region = session.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bca5d-59f3-42aa-a238-d204c1bd8c18",
   "metadata": {},
   "source": [
    "# Deploy Pytorch Model using TorchServe in SageMaker \n",
    "We've now ready to deploy the model for serving recommendations. SageMaker supports the following ways to deploy a model, depending on your use case:\n",
    "\n",
    "* For persistent, real-time endpoints that make one prediction at a time, use SageMaker AI real-time hosting services. For more information, see [Real-time inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html).\n",
    "\n",
    "* Workloads that have idle periods between traffic spikes and can tolerate cold starts, use Serverless Inference. For more informatoin, see [Deploy models with Amazon SageMaker Serverless Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html).\n",
    "\n",
    "* Requests with large payload sizes up to 1GB, long processing times, and near real-time latency requirements, use Amazon SageMaker Asynchronous Inference. For more information, see [Asynchronous inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html).\n",
    "\n",
    "* To get predictions for an entire dataset, use SageMaker AI batch transform. For more information, see [Batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) for inference with Amazon SageMaker AI.\n",
    "\n",
    "Here's a diagram that summarizes the different deployment modes described above:\n",
    "\n",
    "<img src=\"img/sagemaker-deployment-modes.png\" width=\"1000\">\n",
    "\n",
    "\n",
    "For our lab, we'll deploy a [Real-time inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html) for serving inferences using TorchServe with SageMaker SDK. \n",
    "\n",
    "**Note:** The example shown below deploys a pytorch model to an endpoint behind a single GPU instance. For advanced use cases which involves multiple GPUs or GPU instances with torchserve, please refer to [this](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-models-frameworks-torchserve.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789581af-ce15-4076-996d-bfec490bb944",
   "metadata": {},
   "source": [
    "## AWS Deep Learning Containers\n",
    "[AWS Deep Learning Containers](https://github.com/aws/deep-learning-containers) (DLCs) are a set of Docker images for training and serving models in TensorFlow, TensorFlow 2, PyTorch and others. Deep Learning Containers provide optimized environments with TensorFlow, Nvidia CUDA (for GPU instances), and Intel MKL (for CPU instances) libraries and are available in the Amazon Elastic Container Registry (Amazon ECR).\n",
    "\n",
    "To retrieve a specific container image, you can directly reference the ECR URI in the github link, or use SageMaker SDK to return the proper URI based on the frameowork version. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db107541-b8b8-40f7-9cef-020074c4bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find inference container image\n",
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='djl-lmi', # use lmi image until deep learning image is available\n",
    "    region=region,\n",
    "    py_version='py311',\n",
    "    image_scope=\"inference\",\n",
    "    instance_type='ml.p3.2xlarge'\n",
    ")\n",
    "# temporary until sdk updates djl container images fpr py311\n",
    "inference_image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.32.0-lmi14.0.0-cu126'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d251dc4-5fb8-4c9d-9d1a-a9dcb120e5ca",
   "metadata": {},
   "source": [
    "## DJL Serving\n",
    "\n",
    "DJL Serving is a high performance universal stand-alone model serving solution. It takes a deep learning model, several models, or workflows and makes them available through an HTTP endpoint.\n",
    "\n",
    "DJL Serving accepts the following artifacts in your archive:\n",
    "\n",
    "- Model checkpoint: Files that store your model weights.\n",
    "\n",
    "- serving.properties: A configuration file that you can add for each model. Place serving.properties in the same directory as your model file.\n",
    "\n",
    "- model.py: The inference handler code. This is only applicable when using Python mode. If you don't specify model.py, djl-serving uses one of the default handlers.\n",
    "\n",
    "The following is an example of a model.tar.gz structure:\n",
    "\n",
    "\n",
    "```\n",
    "- model_root_dir # root directory\n",
    "  - serving.properties # A configuration file that you can add for each model. Place serving.properties in the same directory as your model file.      \n",
    "  - model.py # your custom handler file for Python, if you choose not to use the default handlers provided by DJL Serving\n",
    "  - model binary files # used for Java mode, or if you don't want to use option.model_id and option.s3_url for Python mode\n",
    "```\n",
    "\n",
    "For more information about DJL Serving, please refer to this [link](https://docs.djl.ai/master/docs/serving/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc29a92-9e64-46ab-8286-89ee6d6f7af1",
   "metadata": {},
   "source": [
    "Download model binary files and add inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e32bf0-8954-4b96-b989-6b5aaed024c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {model_data_url} model/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058e6ae-26a4-4dc3-b6be-76fe2c65601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "rm -rf temp/ && mkdir -p temp && cd temp\n",
    "tar -xvzf ../model/model.tar.gz >/dev/null 2>&1\n",
    "cp -R ../src/* ./ && rm -rd data && rm train.py\n",
    "tar -cvzf ../model/model.tar.gz . >/dev/null 2>&1 && cd .. && rm -rf temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938a841-11bf-4fed-ac89-8fed258874eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"two-tower-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "model_url = S3Uploader.upload(\n",
    "    local_path=\"model/model.tar.gz\",\n",
    "    desired_s3_uri=f\"s3://{bucket}/models/{model_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a260b2-eef7-42c3-9742-51f032c2e69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Model(\n",
    "    image_uri=inference_image_uri,\n",
    "    source_dir=\"src\",\n",
    "    model_data=model_url,\n",
    "    role=role,\n",
    "    sagemaker_session=session,\n",
    "    predictor_cls=Predictor,\n",
    "    env={\n",
    "        \"MASTER_ADDR\" : \"localhost\", \n",
    "        \"MASTER_PORT\" : \"12356\", \n",
    "        \"CUDA_VISIBLE_DEVICES\" : \"0\",\n",
    "        \"LOCAL_RANK\" : \"0\",\n",
    "        \"WORLD_SIZE\" : \"1\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57d813-be52-474b-9e00-cfc03014b549",
   "metadata": {},
   "source": [
    "Deploy the torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb6795-ed3c-4963-bb53-4ba2f9969557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    # instance_type='local_gpu', # uncomment if running in a local mode.\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa8af74-84d0-4d52-8bfe-ad5aad0487e9",
   "metadata": {},
   "source": [
    "The following is the code for [model.py](src/model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3fdd0e-78af-4461-9b0b-08a8f85f396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load src/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4bc91-a995-4bfd-849b-599d0fafdab3",
   "metadata": {},
   "source": [
    "Wait for the endpoint to be ready to serve requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d6fe3-f445-45d3-92ad-8d36f601af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "def wait_for_endpoint(endpoint_name, timeout_minutes=30):\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    max_attempts = timeout_minutes * 2  # Check every 30 seconds\n",
    "    attempts = 0\n",
    "    \n",
    "    while attempts < max_attempts:\n",
    "        response = sagemaker_client.describe_endpoint(\n",
    "            EndpointName=endpoint_name\n",
    "        )\n",
    "        status = response['EndpointStatus']\n",
    "        \n",
    "        if status == 'InService':\n",
    "            return True\n",
    "        \n",
    "        if status in ['Failed', 'OutOfService', 'Deleting']:\n",
    "            raise Exception(f\"Endpoint failed with status: {status}\")\n",
    "            \n",
    "        time.sleep(30)\n",
    "        attempts += 1\n",
    "    \n",
    "    raise TimeoutError(f\"Endpoint did not become ready within {timeout_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a383c9f-32fa-477d-93f8-7c180d98f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_endpoint(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e642357-f1b5-4fb0-9761-fd009b92b4ac",
   "metadata": {},
   "source": [
    "Test the endpoint with random userIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f74587-4210-40b3-b494-e676f2b37a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = {\"inputs\": [1234, 534, 30]} #userIds\n",
    "response = predictor.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29746949-25a3-42fd-8f81-2d26bcc26bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229f0e69-4c96-4f71-8067-ce0d946027c2",
   "metadata": {},
   "source": [
    "Saving model information for next lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107bcdf-7d15-45aa-a135-381780af066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = predictor._get_model_names()[0]\n",
    "model_serving_data_s3_uri = predictor.sagemaker_session.sagemaker_client.describe_model(\n",
    "    ModelName=model_name\n",
    ")['PrimaryContainer']['ModelDataUrl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34673eb6-401b-4004-9c64-28febbb19369",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store model_serving_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68583391-793e-4d17-aee5-34980d9972c9",
   "metadata": {},
   "source": [
    "### Delete Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d57c1-bfaf-4807-8df7-cacfa8fa71a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab1d54-a5e1-4d11-b903-00c9ea58b436",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "Congratulations! You've completed the end to end process of deploying a Pytorch model into Torchserve using SageMaker. \n",
    "In the next lab, we'll explore [Shadow testing](https://docs.aws.amazon.com/sagemaker/latest/dg/shadow-tests.html), a unique feature i SageMaker that allows you to evaluate new model variants alongside your existing production model without impacting live traffic or end users. Go ahead and open [03-sm-inference-shadow-test.ipynb](03-sm-inference-shadow-test.ipynb) and follow the instructions in the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
